\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{LLM vs Scripts: English Texts Complexity Evaluation Methods}
\author{Arseniy Polyakov, Higher School of Economics (HSE)}
\date{January 2025}

\begin{document}

\maketitle
\section{Key Words}
English, text complexity, CEFR, readability formulas, Hugging Face API, LLM, prompt engineering, chain-of-thought, Qwen,
traditional and generative approaches, preprocessing, Telegram bots

\section{Abstract}
The core goal of the article is to compare two methods in English texts complexity evaluation: traditional (a script approach) and generative (with the help of the Large Language Model (LLM)), and analyze which provides the most precise outputs in an optimal time. Quantitative methods are used: function programming in Python and prompt-engineering to count the most wide-spread readability formulas. Moreover, the sample of expert mark-up texts is collected for testing. As for product part, the Telegram chat-bot is provided, which parses different types of media content. As a result, such metrics as Flesch Reading Ease, LIX and Coleman Liau Index evaluated by LLM have become closer to script results (the subtract is approximately in the range from 10 to 20 percent). 

\section{Introduction}
The evaluation of English written texts is a common direction in teaching, psycholinguistics, marketing and search engine optimization (SEO). Using a proper text from the angle of its complexity is a core aspect in powerful teaching and student development, efficient sales via social networks, cites and messengers, and a necessary control of people who have different forms of aphasia. There are few classic metrics that could evaluate an English text complexity from all linguistic levels (phonological, grammatical and lexical). However, they are often counted manually or with the help of NLP programming libraries. The purpose of the research is to collect the most wide-spread quantitative metrics and compare the effectiveness of their counting by using two methods: traditional (Scripts in Python with NLP libraries) and generative (using prompt engineering techniques).   

\section{Related Works}
The field of English texts evaluation started in the middle of the 20th century. The core authors in this domain were Peter Kincaid, Robert Gunning, Meri Coleman and Ta Lin Liau, Harry Mc Laughlin, Edgar A Smith and RJ Senter, they created readability formulas which could evaluate a text by the quantitative scale. Moreover, the important factor in the development of readability metrics are standards of the Common European Framework of References (CEFR) which were provided by Verhelst, Piet Van Avermaet, S Takala, N Figueras, and B North. 
Nowadays, researchers use readability indexes in evaluating students effectiveness in learning foreign languages (Shingo Nahatame, Emilio Matricciani), in theoretical linguistic research studies (Bentz, Christian, Gutierrez-Vasques, 2023), fundamental and practical psycholinguistics, especially in the aspect of aphasia treatment (Telvizian T, Jiang L, 2024).

Traditionally, linguists classify text complexity metrics according to linguistic levels. As for raw texts features, there are word length and sentence length (Brunato, D., De Mattei, L., 2018). Grammatical metrics such as part-of-speech distribution (Brunato, D., De Mattei, L., 2018), the number of notional parts of speech divided to all tokens in the text. for instance, nouns to words (text nominativity), adjectives to words (text descriptivity), verbs to words, adverbs to words, pronouns to words and numerals to words (Deutsch, Jasbi, and Shieber, 2020). On the lexical level there are words and collocations distribution according to CEFR classification (Verhelst, Piet Van Avermaet, 2001), type token ratio (the number of unique words in the text to the whole tokens) (Zakharova E.U., Savina O.U., 2020) and also root type token ration and corrected type token ratio [4, 5]. Furthermore, traditional quantitative metrics such as
\begin{itemize}
    \item Flesh Reading Ease (Kincaid et al., 1975). It has a range from 0 to 100 (where 0 is the metric of the easiest text and 100 of the hardest)
    \[
    FRE = 206.835 - (1.015 * (words / sentences)) - (84.6 * syllables / words)
    \]
    \item Flesh-Kincaid Grade Level (FKGL) (Kincaid et al., 1975). It has a range from 0 to 100 (where 0 is the metric of the easiest text and 100 of the hardest)
    \[ 
    FKGL = (0.39 * (words / sentences)) + (11.8 * (syllables / words)) - 15.59
    \]
    \item Gunning Fog Count (FOG) (Gunning et al., 1952). It indicates a grade level of the text (from 0 to 20, when 0 is the easiest text, 20 is the hardest)
     \[ 
    FOG = (0.4 * ((words/sentences) + 100*(complexwords/words))
    \]
    \item (Simple Measure of Gobbledygook) (Mc Laughlin, 1969). SMOG which has a range from 3 to 10 and more (the average number of years which is needed to understand a proper text)
    \[ 
    SMOG = 3 + \sqrt(pollysyllabicwords)
    \]
    \item Automated Readability Index (ARI). It has a range from 0 to 15 and more (the lowest score symbolizes the easiest text)
    \[
    ARI = 4.71 * (characters/words) + 0.5*(words/sentences) - 21.43
    \]
    \item Spache Formula. Spache formula works with a dataset of basic words. If the text has more unique words (not from this list) it will have a higher Spache score. 
    \[
    Spache = (0.121*averagesentlength) + (0.082*percentuniquewords) + 0.659
    \]
    \item Dale Chall Formula. As Spache Formula it operates a dataset of elementary lexical units and finds a number of words which are out of this list. 
    \[
    Dale  = 0.1579*((100*complexwords/words) + (0.0496*words/sentences))
    \]
    \item Powers Sumner Kearl. This readability formula is more suitable for an elementary school students (a reading age below 10)
    \[
    PSK = (0.0778*averagesentencelength) + (0.0455*syllables) + 2.7971
    \]
    \item Coleman Liau Index. The formula shows a grade which is appropriate for this text (for instance: 8 points shows the 8th grade)
    \[
    CLI = (0.0588*averageletters) - (0.296*averagesentences) - 15.8
    \]
    \item Lasbarhetsindex (LIX). It has a range from 20 to 60 and more (where 20 is the metric of the easiest text, 60 and more of the hardest)
    \[ 
    LIX = (long_words * 100/words) + averagenumberwordspersentence
    \]
    \item RIX. An another version of LIX readability formula
    \[
    RIX = longwords / sentences
    \]
\end{itemize}
\section{Methodology}
The primal goal of the research is to count readability metrics thanks to two ways: traditional (script) by using Python NLP libraries and preprosessing and generative thanks to LLM prompts engineering, compare results and prove or disprove a hypothesis that LLM is appropriate for automatic metrics counting. 

As for the former approach, the core components for metrics such as the number of characters, words, sentences; the average number of characters per word, words per sentence, the quantity of long and polysyllabic words were found (NLTK library for counting words and sentences, regular expressions for punctuation marks, signs and digits extraction to split the sentences into the words in the right way, a self-made function for counting the number of syllables in the word: based on counting vowels and subtraction of vowel digraphs and trigraphs, the "-e" letter at the final position except short articles and pronouns like "he", "the", "she" etc and borrowing especially from French like "cliche", "apostrophe" and so on and the "y" letter at the beginning ("-e" and "y-" letters at the end and at the beginning proper do not form a syllable).  

As for the latter approach, the prompt for LLM (Qwen 2.5 on 72 billion parameters which is launched on CPU with Hugging Face API) was written (see Appendix 1). The pivotal aspects of prompt were used like the topic "counting readability formulas", the instruction "to count appropriate text complexity metrics", the context (enumerating these metrics), the role "An expert in English text evaluation" and a formal markdown for a strict structure of an answer. Furthermore, it is necessary to mention the language detection in the prompt because this research is done only for the English language and all readability formulas with their coefficients which were found in an empirical way do not suit other languages (they will differ). 

Furthermore, a product for evaluating English texts is done. It is a synchronic telegram chat-bot (PytelegramBotApi2, telebot library), which provides a text evaluation for all types of potential media content where this aspect will be necessary including parsing videos and audios (Whisper API transcribation, audio files are transcribated with the ffmpeg binary file and the Pydub library), text documents with pdf, doc, docx, txt extensions (for .pdf documents PyPDF2 library is used for parsing data, for .doc, .docx and .txt Tika library with Java 7+) and messages proper. There are several handlers which are integrated in chat-bot's scenario to process mistakes (especially when the input text is written in other languages except English or a user sends other content types like stickers, forms, etc or documents with non-text extensions like pptx, xlsx, etc). The bot's answer is the LLM or script descriptional statistics based on readability formulas. 

The chat-bot with script readability formulas and LLM prompts is freely avaliable online on GitHub repository (the link will be provided after the paper deanonymization). 

\section{Data}
To collect the data, one of the most commonly used sources was chosen, British Council. It is one of the most popular organizations in their international cultural and educational events. Furthermore, it provides a cite with various texts on different topics which are distinguished by their complexity, volume, style and context. Moreover, texts that are marked with the help of experts according to the Common European Framework of References (CEFR) from A1 to C1 levels are presented here. Ten texts (two for each complexity level) were chosen as a fundamental data for the experiment (A1 level: "A Poster to Work" and "Holiday Home Adverts", A2 level: "A Message to a New Friend" and "Study Skills Tips", B1 level: "A Travel Guide" and "Robots Teachers", B2 level: "Asteroids" and "Millenials in the Workspace" and C1 level: "Horror Films Cliche" and "Managing a Problem"). 

\section{Results}
Results of script and generative approaches were viewed from two core aspects: program time and data output. As for the former aspect, the Python module "time" from the standard library was used. In all test cases a script works faster than LLM (see Table 1):
\begin{center}
\caption{Table 1. LLM and Script's Program Time (sec)}
\begin{tabular}{ c c c }
Texts & \textbf{LLM (Qwen 2.5)} & \textbf{Script} \\
Text 1 (A1) & 0.80 & \textbf{0.1} \\
Text 2 (A1) & 9.82 & \textbf{0.07} \\
Text 3 (A2) & 10.17 & \textbf{0.1} \\
Text 4 (A2) & 12.73 & \textbf{0.12} \\
Text 5 (B1) & 16.05 & \textbf{0.14} \\
Text 6 (B1) & 15.5 & \textbf{0.09} \\
Text 7 (B2) & 9.12 & \textbf{0.12} \\ 
Text 8 (B2) & 18.32 & \textbf{0.12} \\ 
Text 9 (C1) & 8.43 & \textbf{0.1} \\ 
Text 10 (C1) & 9.4 & \textbf{0.18} \\
Average Time & 11.04 & \textbf{0.11} \\
\end{tabular}
\end{center}
Moreover, readability formulas were counted thanks to prompt engineering and script functions. To compare the subtracts of LLM results (to compare with the script ones) for each metric were chosen (in percents). The average percent (the module of it) of a LLM and script outputs were found for all formulas in a sample (see Table 2):
\begin{center}
\caption{Table 2. The LLM outcomes (the difference with the script, in percents)}
\begin{tabular}{ c c c }
\textbf{Readability Formulas} & \textbf{Percent} \\
Flesch Reading Ease (FRE) & \textbf{12.21} \\
Flesch Kincaid Grade Level (FKGL) & 21.52 \\
Gunning Fox & 51.25 \\
SMOG & 59.6 \\
Automated Readability Formula (ARI) & 38.16 \\
Spache Formula & 28.74 \\
Dale Chall Formula & 29.17 \\ 
Powers Sumner kearl & 57.05 \\ 
Coleman Liau Index & \textbf{12.13} \\ 
LIX & \textbf{19.04} \\ 
RIX & 148.32 \\
\end{tabular}
\end{center}
Analyzing the table, it is possible to underline that Flesch Reading Ease (FRE), Coleman Liau Index and LIX have the smallest score in the sample, therefore, they are more precise in counting with a generative method (based on the sample of 10 texts from British Council). 
Moreover, two types of prompts (the extended prompt with 11 metrics and a short prompt only with three which are the most efficient (see Appendix 2)) were used to find a text level according to CEFR. For this task the chain-of-thought prompting was integrated (finding an answer based on reasoning). The results of two prompts were compared with an original mark-up from British Council (see Table 3):   
\begin{center}
\caption{Table 3. Full Prompt and Short Prompt Comparison}
\begin{tabular}{c c c c}
    Texts & \textbf{Original Mark-Up} & \textbf{Full Prompt} & \textbf{Short Prompt} \\
    Text 1 (A1) & A1 & A2 & B1 \\
    Text 2 (A1) & A1 & B1 & B1 \\
    Text 3 (A2) & A2 & B1 & B1 \\
    Text 4 (A2) & A2 & B1 & B1 \\
    Text 5 (B1) & B1 & \textbf{B1} & \textbf{B1} \\
    Text 6 (B1) & B1 & B2 & B2 \\
    Text 7 (B2) & B2 & \textbf{B2} & C1 \\
    Text 8 (B2) & B2 & \textbf{B2} & \textbf{B2} \\
    Text 9 (C1) & C1 & B2 & B1 \\
    Text 10 (C1) & C1 & B2 & B2 \\
\end{tabular}
\end{center}
According to the table, 3 out of 10 results of a full prompt are the same like an expert mark-up and 2 out of 10 results are of a short prompt. Moreover, a full prompt has more approximate outcomes (for instance: in Text 1 and Text 9 in spite of a short prompt). That is why, it is possible to mention that full chain-of-thought prompts with variable metrics in the context are more efficient in text evaluation instead of short ones (based on the 10 texts from the British Council). 

\section{Discussion}
There are several points which are needed to mention in the aspect of a text complexity evaluation. At first, the final result depends on the way how the number of syllables and words were counted (what linguistic rules the formula for counting syllables contains, was the stop-words included (and what kinds) in the number of words or not), what datasets (for Dale Chall and Spache Formulas) are used. Secondly, the standard benchmark (in this research there are British Council texts) might be verified with experts in English-teaching and methodology. Last but not least, it is a volume of a test sample and a number of tests which are provided for more precise outcomes and a model which is used to count metrics, these factors also influence the result.

\section{Conclusion}
In general, LLM with dozens of parameters (like Qwen 2.5 on 72 billion) could be used for counting some English complexity metrics, for instance, Flesch Reading Ease, Coleman Liau Index and LIX, other tested metrics are counted with hallucinations. As for finding CEFR level, the most effective prompt technique was a chain-of-thought with a large number of metrics which are used in context (more than 10). Based on the experiment this prompt showed better outcomes instead of a short one.
As for future work, the number of formulas could be expanded (for instance, by such metrics as FORECAST, Linsear Write) and a full-prompt should be corrected for better results in CEFR level finding. As for product part, the chat-bot will have two branches: full statistics by the formulas (according to script and LLM results) and short ones with CEFR level (also with two parts: script and LLM). Moreover, the number of tests and texts would be enlarged, texts should be borrowed from different well-known sources and an experiment could be provided in a comparative aspect with several LLM's. 

\section{References}
\begin{enumerate}
\item Brunato, D., De Mattei, L., Dell’Orletta, F., Iavarone, B., and Venturi, G. (2018). “Is this sentence difficult? Do you agree?,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (Brussels: Association for
Computational Linguistics), 2690–2699.

\item Telvizian T, Jiang L, et al. (2024) Biomedical text readability after hypernym substitution with fine-tuned large language models. PLOS Digit Health 3(4): e0000489. https://doi.org/10.1371/journal.pdig.0000489

\item Bentz, Christian, Gutierrez-Vasques, Ximena, Sozinova, Olga and Samardžić, Tanja. "Complexity trade-offs 
and equi-complexity in natural languages: a meta-analysis" Linguistics Vanguard, vol. 9, no. s1, 2023, pp. 9-25.

\item The method of detecting the author’s age according to the indexes of readability and lexical diversity / A.A. Sobolev, A.M. 
Fedotova, A.V. Kurtukova [and others] Proceedings of Tomsk State University of Control Systems and Radioelectronics. 2022. Vol. 25, №2.: 45-52.

\item Zakharova E.U., Savina O.U. Lexical diversity of texts and ways how to measure it // Tumen State University Herald. Humanities. 2020. Т. 6, №1(21).: 20-34.

\item Peter Kincaid, Robert P Fishburne Jr, Richard L, Rogers, and Brad S Chissom. 1975. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Technical report, Naval
Technical Training Command Millington TN Research Branch.

\item Robert Gunning et al. 1952. Technique of clear writing

\item Harry Mc Laughlin. 1969. Smog grading-a new readability formula. Journal of reading, 12(8):639–646

\item Meri Coleman and Ta Lin Liau. 1975. A computer readability formula designed for machine scoring. Journal of Applied Psychology, 60(2):283.

\item Edgar A Smith and RJ Senter. 1967. Automated readability index. AMRL-TR. Aerospace Medical Research Laboratories (US), pages 1–14.

\item Verhelst, Piet Van Avermaet, S Takala, N Figueras, and B North. 2001. Common European Framework of Reference for Languages: learning, teaching, assessment. Cambridge University Press.

\item Deutsch, Jasbi, and Shieber Linguistic Features for Readability Assessment, 2020.
\end{enumerate}

\section{Appendix 1. The Full Prompt}
\begin{verbatim}
"You are an expert in English texts evaluation.\n"
f"Count readability metrics in the English text {message} 
and choose a text level according to CEFR. 
Base your answer on the metrics you find\n"
"Metrics:\n" 
    FRE (Flesch Reading Ease)\n" 
    FKGL (Flesch-Kincaid Grade Level)\n" 
    Gunning Fox Index\n" 
    SMOG (Simple Measure of Gobbledygook)\n" 
    Automated Readability Formula (ARI)\n" 
    Spache Formula\n" 
    Dale Chall Formula\n" 
    Powers Sumner Kearl\n" 
    Coleman Liau Index\n" 
    LIX\n" 
    RIX\n" 
"Answer this way\n" 
"This text has:\n" 
    "FRE <value>\n" 
    "FKGL <value>\n" 
    "Gunning Fox <value>\n" 
    "SMOG <value>\n" 
    "Automated Readability Formula <value>\n" 
    "Spache Formula <value>\n" 
    "Dale Chall Formula <value>\n" 
    "Powers Sumner Kearl <value>\n" 
    "Coleman Liau Index <value>\n"  
    "LIX <value>\n" 
    "RIX <value>\n" 
    "<CEFR level>\n" 
"If the text is written in another language 
say that you cannot count metrics"
\end{verbatim}
\section{Appendix 2. The Short Prompt}
\begin{verbatim}
"You are an expert in English texts evaluation.\n"
"Count readability metrics FRE, Coleman Liau Index and LIX 
and choose an English text level according to CEFR callssifition\n" 
f"Text: {message}" 
"Metrics:\n"
    FRE (Flesch Reading Ease)\n" 
    Coleman Liau Index\n" 
    LIX\n" 
"Answer this way\n" 
    "FRE: <value>\n" 
    "Coleman Liau Index: <value>\n" 
    "LIX: <value>\n"
"This text has <CEFR level> and provide a short explanantion 
(no more than 100 words) based on metrics interpretation\n"
"If the text is written in another language say 
that you can count metrics only in English texts"
\end{verbatim} 
\end{document}
