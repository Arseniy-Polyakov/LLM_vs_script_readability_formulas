\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{mathtools}
\title{Automatic English Texts Evaluation}
\author{Arseniy Polyakov, HSE Saint Petersburg}
\date{December 2024}

\begin{document}

\maketitle

\section{Key Words}
English, Text Complexity, CEFR, Quantitative Metrics, Hugging Face API, LLM, Prompt engineering, chain-of-thought,
Traditional and generative approaches, NER, Topic Modeling, Preprocessing, Linguistic layers, Telegram bots

\section{Introduction}
English text evaluation is a wide-spread task in the fields of linguistic didactics, methodology, and computational linguistics. 
The appropriate level of a text helps in the building and modeling of useful lessons and exams. The Common European Framework of References (CEFR) provides a plural gradation of English texts levels (from A1 - the easiest to the C2 - proficient). This metric system is used all over the world in international exams, tasks, texts and books adaptation and evaluation systems, which help to mark students' works. 
However, there is no particular methodology for evaluating English texts and finding the appropriate and accurate complexity level. That is why the core task is to research popular quantitative metrics, collect it for the English text complexity and compare two approaches: generative (by using LLM) and traditional (script).

\section{Literature Review}
Nowadays, there are several quantitative metrics that are used to evaluate the English text. It could be divided according to the linguistic levels.
\medskip 

General information [3]:
\begin{enumerate}
    \item The number of sentences in the text
    \item The number of words in the text
    \item The number of words (without stopwords) in the text
    \item The average number of words in the sentence
    \item The average number of words (without stopwords) in the text
\end{enumerate}
\medskip 

Phonological level [7]:
\begin{enumerate}
    \item The number of syllables in the text
    \item The average number of syllables in the word
    \item The average number of syllables in the sentence
    \item The number of 1-syllable, 2-syllable, 3-syllable and 4-syllable words in the text
    \item The average number of 1-syllable, 2-syllable, 3-syllable and 4-syllable words in the sentence
\end{enumerate}
\medskip 

Grammatical level [8]:
\begin{enumerate}
    \item The number of nouns in the text. 
    \item The number of adjectives in the text. 
    \item The number of verbs in the text.
    \item The number of adverbs in the text.
    \item The number of pronouns in the text.
    \item The number of numerals in the text.
    \item Text descriptivity: the number of adjectives divide to the whole number of all words in the text
    \item Text nominativity: the number of nouns divide to the whole number of all words in the text
\end{enumerate}
\medskip 

Lexical level [1, 2]:
\begin{enumerate}
    \item The number of A1, A2, B1, B2, C1 words in the text
    \item The average number of A1, A2, B1, B2, C1 words in the sentences
    \item The number of A1, A2, B1, B2, C1 collocations in the text
    \item The average number of A1, A2, B1, B2, C1 collocations in the sentences
    \item Type token ratio (TTR): 
    \[
    TTR = (uniquewords / total words)
    \]
    \item Root Type Token Ratio (RTTR)
    \[
    RTTR = (uniquewords / \sqrt(totalwords)
    \]
    \item Corrected Type Token Ratio (CTTR)
    \[ 
    CTTR = uniquewords / \sqrt(2 * totalwords)
    \]
\end{enumerate}
\medskip 

Statistical Metrics [4, 5, 6]:
\begin{itemize}
    \item Flesh Reading Ease which has a range from 0 to 100 (where 0 is the metric of the easiest text and 100 of the hardest)
    \[
    FRE = 206.835 - (1.015 * (words / sentences)) - (84.6 * syllables / words)
    \]
    \item Flesh-Kincaid Grade Level (FKGL) which has a range from 0 to 100 (where 0 is the metric of the easiest text and 100 of the hardest)
    \[ 
    FKGL = (0.39 * (words / sentences)) + (11.8 * (syllables / words)) - 15.59
    \]
    \item Lasbarhetsindex (LIX) which has a range from 20 to 60 and more (where 20 is the metric of the easiest text, 60 and more of the hardest)
    \[ 
    LIX = (words / sentences) + (polysyllabicwords * 100 / words), 2)
    \]
    \item (Simple Measure of Gobbledygook) SMOG which has a range from 3 to 10 and more (the average number of years which is needed to understand a proper text) \[ 
    SMOG = 3 + \sqrt(pollysyllabicwords)
    \]
\end{itemize}
Coefficients of these metrics are empirical and actual only for the English language (if there is an other language it could be differ). 
There are other quantitative metrics like Average Reading Level 
Consensus, Automated Readability Index, Gunning Fog Index and so on, however, we will analyze only the four above because they are the most widely used in a text evaluation.

\section{Traditional and Generative Approaches}
There are two approaches in the English text evaluation that are compared in the article: traditional (scripts in Python) and generative (prompt engineering). It will be a comparative analysis of it based on such components as: program time, a final result (at first, a text level according to CEFR), full output data and its accuracy. 

\section{Training Sample}
There is a training sample which consists of 10 marked English texts (two texts for each level from A1 to C1).
The texts are borrowed from this site[9]. 

\section{Methodology. Traditional (Script) Approach}
The first is the text pre-prosessing. It contains several basic steps: segmentation (sentence division), tokenization (division by tokens, in this particular case notional and functional parts of speech), avoiding punctuation marks and stopwords (the words which have their grammatical but not lexical meaning, in the majority of cases there are functional parts of speech), lemmatization (making the normal form of notional verbs), NER (Named Entity Recognition, extracting personal names and toponyms in the text) and converting to the lower case. 

Segmentation and tokenitation are provided by the NLTK Python library, stopwords are deleted by the list comprehension comparison based by a generated list of stopwords by LLM (GPT 4o mini), lemmatization and NER are also done with NLTK, punctuation marks are avoided thanks to regular expressions. 

General metrics about the whole number of sentences and words (with stopwords and without it.), its average numbers in sentences are made by counting the length of data structures. 

In the aspect of phonological metrics the function of automatic syllable counting is provided (the algorithm is to count all vowel letters which are syllable-building and remove digraphs and trigraphs which make only one sound and, therefore, one syllable). 

Grammatical metrics are made by POS-tagging (Parts-of-Speech Tagging) by NLTK function. According to the POS-tagging text nominativity and descriptivity are counted and the number of notional parts of speech (nouns, verbs, adjectives, adverbs, pronouns and numerals) is found.

Lexical level is based in CEFR gradation of words and collocations (from A1 to C1 levels). The data is based on "The Oxford 5000" dataset (the dataset of the most common
English words and phrases that are tagged with labels from A1 to C1). Furthermore, the most widespread lexical metrics are mentioned like TTR, RTTR, CTTR. 
\medskip

Moreover, topic modeling (top 5 common topics in the text) is provided. It is a native algorithm which is based on the quantitative approach of counting the most common words by topics (there are 18 topics like "Animals", "People", "Work and Business", "Travel", etc. which are borrowed from "The Oxford 5000" dataset too). 

\section{Methodology. Generative Approach}
For the generative approaches Large Language Model (LLM) Qwen 2.5 on 72 Billions of parameters is used (CPU, Hugging Face API integration). There are two types of generative approach: the former is to duplicate a script scenario by the prompt and compare program time, the output by the completeness and accuracy. The latter is to use chain-of-thought prompting based on quantitative metrics with clear benchmarks (FRE, FKGL, LIX and SMOG) for model reasoning and choosing an appropriate answer (A1, A2, B1, B2, C1). 
In general, you can see differences in program time (Python module timeit is used), level evaluation and full output data (see Appendix 3): 
\begin{center}
\begin{tabular}{ c|c|c }
\hline 
 & \textbf{LLM (Qwen 2.5)} & \textbf{Script} \\
Program Time (Text 1, A1 level) (sec) & 0.0252 & \textbf{0.0149} \\
Program Time (Text 2, A1 level) (sec) & 0.0239 & 0.0145 \\
Program Time (Text 3, A2 level) (sec) & 0.0213 & \textbf{0.0133} \\
Program Time (Text 4, A2 level) (sec) & 0.0257 & \textbf{0.015} \\
Program Time (Text 5, B1 level) (sec) & 0.0183 & \textbf{0.0133} \\
Program Time (Text 6, B1 level) (sec) & 0.0102 & \textbf{0.0079} \\
Program Time (Text 7, B2 level) (sec) & \textbf{0.0079} & \textbf{0.0079} \\ 
Program Time (Text 8, B2 level) (sec) & \textbf{0.0079} & 0.0092 \\ 
Program Time (Text 9, C1 level) (sec) & \textbf{0.007905} & 0.007947 \\ 
Program Time (Text 10, C1 level) (sec) & \textbf{0.0081} & 0.007947 
\end{tabular}
\end{center}
Analyzing this table, we could create a hypothesis that the text complexity and program time are connected (LLM results are better on complex texts than on simple ones). 
\par

As for level choosing two possible chain-of-thought prompting is compared with manual mark-up:
\begin{center}
\begin{tabular}{ c|c|c }
\hline 
Texts & \textbf{LLM (Qwen 2.5) Chain of Thought} & \textbf{Text Level Proper} \\
Text 1 & B1 (first test) \textbf{A1} (second test) & \textbf{A1} \\
Text 2 & B1 (first test) \textbf{A1} (second test) & \textbf{A1} \\
Text 3 & \textbf{A2} (first test) & \textbf{A2} \\
Text 4 & \textbf{A2} (first test) & \textbf{A2} \\
Text 5 & \textbf{B1} (first test) & \textbf{B1} \\
Text 6 & \textbf{B1} (first test) & \textbf{B1} \\
Text 7 & C1 & B2 \\ 
Text 8 & \textbf{B2} (first test) & \textbf{B2} \\ 
Text 9 & B2 (first test) & C1 \\ 
Text 10 & B2 (first test) & C1 \\ 
\end{tabular}
\end{center}
As the table shows, it is possible to approximately evaluate the percentage of LLM hallucinations (~30-50\%). 
\par 

The last but not the least test is connected with full statistics comparison. One of the comparative and contrastive tables is provided in Appendix 3. To summarize it, the majority of quantitative metrics are different, however, interpretation of the text level with chain-of-thought prompt strategy is more effective. 

\section{Minimal Vital Product (MVP). Telegram Bot}
Speaking about automatic English texts evaluation it is possible to point two ways of clients' interaction. The former one is a decision to find a short precise CEFR level (for instance, just an A2 level and a short comment about it). The latter is a full and detailed statistics on all metrics classified on the levels and presented in an interactive format (like dashboards). As for the first variant, Telegram bot was chosen. It is done with PyTelegramBotApi2 Python library, especially its synchronous module Telebot. The core feature of this product is a multi-format parsing: a chat-bot could analyze videos, audio, text documents (in .pdf, .doc, .docx, .txt formats) and telegram messages proper. Video and audio are transcribed from the Whisper library (by OpenAI) based on the PyTorch framework. Moreover, before Whisper transcription telegram audio files are converted from .oga to .mp3 format thanks to the pydub library and ffmpeg binary files. Documents are analyzed with PyPDF2 library (for .pdf files) and tika framework (Java extension for text documents convertion). As a result, MVP is a telegram chat-bot with video, audio, text documents and messages parsing, LLM-module based on chain-of-thought prompting (with reasoning in 4 precise quantitative metrics: FRE, FKGL, LIX and SMOG) and a strict answer length control (approximately 10-15 words) [see Appendix 5].

\section{Results} 
After the research and making MVP, it is possible to underscore some hypotheses and outcomes. Firstly, chain-of-thought prompting (Qwen 2.5 72B) is an effective way to estimate a text level (with accuracy of ~50-70\%). It is a necessary for interpretation of the severals metrics (no one has a precise range which is equvalent to the CEFR classification). Secondly, program time (according to timeit Python module) less in script programs than in LLM (in order to texts with a low complexity level: A1), practically the same (in order to texts with intermediate levels: A2 and B1) and a little bit higher with sophisticated papers (B2 level and more). Thanks to this initial data we could create a hypothesis that programs on LLM will work faster with complex texts (not even with the long ones) than with short. Thirdly, there are a lot of hallucinations with full statistics which is provided by LLM (Qwen 2.5). It is hard to verify it because the fundamental data is under control (it is unreal to know precise data: stopwords, levels classifications of words and collocations, the algorithm of the syllable division which are influence the final statistics). On the other hand, LLM provides a precise and vast topic modeling based on the 
\section{Future Works}
In future it is necessary to continue the research of quantitative metrics, collecting and analyzing them using computer methods. Moreover, the test sample should be expand (in several times) to proof or disprove a hypothesis of program time correlation with text complexity. Furthermore, it is possible to expand a prompt with few-shot strategy to point how to count several metrics or integrate a mixed approach (to use own marked datasets of stopwords, words and collocations according to the CEFR levels and function calling, counting syllables). As for the client part, an output json structure could be viewed as interactive dashboards (for instance, by Dash Python framework based on Plotly library) or a full-stack site. 
\section{References}
\begin{thebibliography}{10}
\bibitem{}
Zakharova E.U., Savina O.U. Lexical diversity of texts and ways how to measure it // Tumen State University Herald. Humanities. 2020. Т. 6, №1(21).: 20-34.
\bibitem{}
Kazachkova M.B., Galimova H.N. Lexical diversity as a text evaluation metric // Vestnik of the Mari State University. 2021. Vol. 15, №3(43).: 384-390.
\bibitem{}
Karaulov U.N. Russian and the language personality. Pub. 7-е. Moscow: Publishment LKI, 2010. 264 p.
\bibitem{}
The method of detecting the author’s age according to the indexes of readability and lexical diversity / A.A. Sobolev, A.M. 
Fedotova, A.V. Kurtukova [and others] Proceedings of Tomsk State University of Control Systems and Radioelectronics. 2022. Vol. 25, №2.: 45-52.
\bibitem{}
Mikk Ya. A. The methodology of evaluation text complexity // Psychology questions. 1975. №3.: 147-155.
\bibitem{}
Solnyshkina M.I., Kiselnikov A.S. Text complexity: education steps in domestic applied linguistics // Tomsk State University Journal. Philology. №6(38).: 86-99.
\bibitem{}
Kupriyanov R.V. Cognitive complexity measures for educational texts: Empirical validation of linguistic parameters / R.V. Kupriyanov, O.V. Bukach, O.I. Aleksandrova // Russian Journal of Linguistics. 2023. Vol. 27, №3.: 641-662.
\bibitem{}
Morozov D.A. Text complexity and linguistic features: Their correlation in English and Russian / D.A. Morozov, A.V. Glazkova, B.L. Iomdin // Russian Journal of Linguistics. 2022. Vol. 26, №2.: 426-448.
\bibitem{}
English Texts (by levels from A1 to C1)
\href{https://lingua.com/english/reading/}
\end{thebibliography}

\section{Appendix 1. Testing Sample} 
\textbf{Text 1. A1 level}
\par
\textbf{My Wonderful Family} 
\par
I live in a house near the mountains. I have two brothers and one sister, and I was born last. My father teaches mathematics, and my mother is a nurse at a big hospital. My brothers are very smart and work hard in school. My sister is a nervous girl, but she is very kind. My grandmother also lives with us. She came from Italy when I was two years old. She has grown old, but she is still very strong. She cooks the best food!

My family is very important to me. We do lots of things together. My brothers and I like to go on long walks in the mountains. My sister likes to cook with my grandmother. On the weekends we all play board games together. We laugh and always have a good time. I love my family very much.
\medskip

\textbf{Text 2. A1 level}
\par
\textbf{Preparing food}
\par
Jack was hungry. He walked to the kitchen. He got out some eggs. He took out some oil. He placed a skillet on the stove. Next, he turned on the heat. He poured the oil into the skillet. He cracked the eggs into a bowl. He stirred the eggs. Then, he poured them into the hot skillet. He waited while the eggs cooked. They cooked for two minutes. He heard them cooking. They popped in the oil.

Next, Jack put the eggs on a plate. He placed the plate on the dining room table. Jack loved looking at his eggs. They looked pretty on the white plate. He sat down in the large wooden chair. He thought about the day ahead. He ate the eggs with a spoon. They were good.

He washed the plate with dishwashing soap. Then, he washed the pan. He got a sponge damp. Finally, he wiped down the table. Next, Jack watched TV.
\medskip

\textbf{Text 3. A2 Level}
\par
\textbf{At school}
\par
Lucas goes to school every day of the week. He has many subjects to go to each school day: English, art, science, mathematics, gym, and history. His mother packs a big backpack full of books and lunch for Lucas.

His first class is English, and he likes that teacher very much. His English teacher says that he is a good pupil, which Lucas knows means that she thinks he is a good student.

His next class is art. He draws on paper with crayons and pencils and sometimes uses a ruler. Lucas likes art. It is his favorite class.

His third class is science. This class is very hard for Lucas to figure out, but he gets to work with his classmates a lot, which he likes to do. His friend, Kyle, works with Lucas in science class, and they have fun.

Then Lucas gets his break for lunch. He sits with Kyle while he eats. The principal, or the headmaster as some call him, likes to walk around and talk to students during lunch to check that they are all behaving.

The next class is mathematics, which most of the students just call math. Kyle has trouble getting a good grade in mathematics, but the teacher is very nice and helpful.
\medskip

\textbf{Text 4. A2 Level}
\par
\textbf{The city where I live}
\par
My name is Clark, and I will tell you about my city.

I live in an apartment. In my city, there is a post office where people mail letters. On Monday, I go to work. I work at the post office. Everyone shops for food at the grocery store. They also eat at the restaurant. The restaurant serves pizza and ice cream.

My friends and I go to the park. We like to play soccer at the park. On Fridays, we go to the cinema to see a movie. Children don't go to school on the weekend. Each day, people go to the hospital when they are sick. The doctors and nurses take care of them. The police keep everyone safe. I am happy to live in my city.
\medskip

\textbf{Text 5. B1 Level}
\par
\textbf{The Empire State Building}
\par
When exploring New York City, there are several different options for activities during a day trip. Some visitors come to see a show, visit art museums, or simply to shop in many of the city’s high-end retailers. However, many tourists simply come to New York City for the sightseeing. One of the most visited landmarks in New York City is the Empire State Building.

The Empire State Building, constructed in 1931, is a 102-story skyscraper, the ninth highest building in the world, and the fourth tallest structure in the United States. It is located in Midtown, Manhattan. This skyscraper is an iconic symbol of the city, having been featured in over 90 popular movies (as of 2018) throughout film history. Tourists come from all over the world to visit this building and view the city from its famous observation decks.

Matthew, an enthusiast of historic buildings, was excited for this trip to New York City because he has always appreciated architectural design. Matthew purchased a ticket that granted him access to beautiful 360-degree views of the city. The ticket included an elevator ride that stopped at two different vantage points, one at the 86th floor and the other at the rooftop observatory on the 102nd floor. From these observation decks, Matthew took incredible photographs of the entire New York City skyline. The rooftop views granted Matthew perfect aerial perspectives of Central Park, the Brooklyn Bridge, Times Square, the Statue of Liberty, and many other important city landmarks.
\medskip

\textbf{Text 6. B1 Level}
\par
Yesterday, Stephen returned from a trip to Washington, D.C., the capital of the United States. His visit took place during the week prior to the Fourth of July. Logically, there were many activities and celebrations in town in preparation for Independence Day. During his stay in the city, Stephen visited a lot of important historical sites and monuments, and he left with a deeper understanding of the political history of the United States.

Stephen spent a lot of time outdoors exploring the important monuments surrounding Capitol Hill. Of course, he saw the White House from its outside gate at 1600 Pennsylvania Avenue. Stephen also visited the Washington Monument, the Jefferson Memorial, and the Lincoln Memorial. These statues and pavilions are dedicated to former U.S. presidents. They commemorate the contributions that these leaders made throughout American history. Washington, D.C. also has several war memorials dedicated to fallen soldiers during the major wars of the 20th century.

Away from the Capitol Hill area, Washington, D.C. has many museums and art galleries worth visiting. Stephen enjoyed his visit to Washington, D.C. because he learned a lot more about American history after touring each important landmark.
\medskip

\textbf{Text 7. B2 Level}
\par
\textbf{Human body parts and organs}
\par
It goes without saying that humans (mammals identifiable as those that stand upright and are comparatively advanced and capable of detailed thought) have pretty remarkable bodies, given all that they've accomplished. (Furthermore, an especially intelligent human brain produced this text!) To be sure, humans have overcome predators, disease, and all sorts of other obstacles over thousands of years.

To fully understand and appreciate these accomplishments, let's take at some of the most well-known parts of the human body!

The head, or the spherical body part that contains the brain and rests at the top of the human body, has quite a few individual organs and body parts on it. (It should quickly be mentioned that hair occupies the space on top of the head, and the ears, the organs responsible for hearing, are located on either side of the head.) From top to bottom, the eyebrows, or horizontal strips of hair that can be found above the eye, are the first components of the head. The eyes are below them, and are round, orb-like organs that allow humans to see.

The eyes make way for the nose, or an external (sticking-out) organ that plays an important part in the breathing and bacteria-elimination processes. Below that is the mouth, or a wide, cavernous organ that chews food, removes bacteria, helps with breathing, and more. The mouth contains teeth, or small, white-colored, pointed body parts used to chew food, and the tongue, or a red-colored, boneless organ used to chew food and speak.

The neck is the long body part that connects the head to the chest (the muscular body part that protects the heart and lungs), and the stomach, or the part of the body that contains food and liquid-processing organs, comes below that.

The legs are the long, muscular body parts that allow humans to move from one spot to another and perform a variety of actions. Each leg contains a thigh (a thick, especially muscular body part used to perform strenuous motions; the upper part of the leg) and a calf (thinner, more flexible body part that absorbs the shock associated with movement; the lower part of the leg). Feet can be found at the bottom of legs, and each foot is comprised of five toes, or small appendages that help balance.

Arms are long, powerful body parts that are located on either side of chest, below the shoulders;arms are comprised of biceps (the thicker, more powerful upper portion), and forearms (the thinner, more flexible lower portion). Hands, or small, gripping body parts used for a tremendous number of actions, are at the end of arms. Each hand contains five fingers, or small appendages used to grip objects.

The aforementioned shoulders are rounded body parts that aid arms' flexibility. One's back is found on the opposite side of the stomach, and is a flat section of the body that contains important muscles that're intended to protect the lungs and other internal organs, in addition to helping humans perform certain motions and actions.
\medskip

\textbf{Text 8. B2 Level}
\par
\textbf{Las Vegas}
\par
Last April, John took a trip to Las Vegas, Nevada. Las Vegas is a popular destination in the western portion of the United States. The town is most popular for its casinos, hotels, and exciting nightlife.

In downtown Las Vegas, John spent a lot of time on The Strip, which is a 2.5 mile stretch of shopping, entertainment venues, luxury hotels, and fine dining experiences. This is probably the most commonly visited tourist area in the city. The Strip at night looks especially beautiful. All of the buildings light up with bright, neon, eye-catching signs to attract visitor attention.

A stay in Las Vegas can feel similar to a visit to many popular cities worldwide. Many of the hotels have miniature versions of important international sites and monuments. These famous landmarks include the Eiffel Tower, Venice, and even ancient Rome.

One day, John took a side trip outside of the city to visit the Grand Canyon, one of the Seven Wonders of the Natural World. The canyon offers a breathtaking view of Nevada’s ridges and natural landscape. John especially liked the canyon because it was removed from all of the noise and movement in downtown Las Vegas.

John had a great time during his trip to Las Vegas. He did not win a lot of money in the casinos. However, he managed to see a lot of amazing sites during his visit to this city that never sleeps.
\medskip

\textbf{Text 9. B2 Level}
\par
\textbf{Spanish Flu Pandemic of 1918}
\par
The deadliest virus in modern history, perhaps of all time, was the 1918 Spanish Flu. It killed about 20 to 50 million people worldwide, perhaps more. The total death toll is unknown because medical records were not kept in many areas.

The pandemic hit during World War I and devastated military troops. In the United States, for instance, more servicemen were killed from the flu than from the war itself. The Spanish flu was fatal to a higher proportion of young adults than most flu viruses.

The pandemic started mildly, in the spring of 1918, but was followed by a much more severe wave in the fall of 1918. The war likely contributed to the devastating mortality numbers, as large outbreaks occurred in military forces living in close quarters. Poor nutrition and the unsanitary conditions of war camps had an effect.

A third wave occurred in the winter and spring of 1919, and a fourth, smaller wave occurred in a few areas in spring 1920. Initial symptoms of the flu were typical: sore throat, headache, and fever. The flu often progressed rapidly to cause severe pneumonia and sometimes hemorrhage in the lungs and mucus membranes. A characteristic feature of severe cases of the Spanish Flu was heliotrope cyanosis, where the patient’s face turned blue from lack of oxygen in the cells. Death usually followed within hours or days.

Modern medicine such as vaccines, antivirals, and antibiotics for secondary infections were not available at that time, so medical personnel couldn’t do much more than try to relieve symptoms.

The flu ended when it had infected enough people that those who were susceptible had either died or developed immunity.
\medskip

\textbf{Text 10. Level C1}
\par
\textbf{The Environment}
\par
In our modern world, there are many factors that place the wellbeing of the planet in jeopardy. While some people have the opinion that environmental problems are just a natural occurrence, others believe that human beings have a huge impact on the environment. Regardless of your viewpoint, take into consideration the following factors that place our environment as well as the planet Earth in danger.

Global warming or climate change is a major contributing factor to environmental damage. Because of global warming, we have seen an increase in melting ice caps, a rise in sea levels, and the formation of new weather patterns. These weather patterns have caused stronger storms, droughts, and flooding in places that they formerly did not occur.

Air pollution is primarily caused as a result of excessive and unregulated emissions of carbon dioxide into the air. Pollutants mostly emerge from the burning of fossil fuels in addition to chemicals, toxic substances, and improper waste disposal. Air pollutants are absorbed into the atmosphere, and they can cause smog, a combination of smoke and fog, in valleys as well as produce acidic precipitation in areas far away from the pollution source.

In many areas, people and local governments do not sustainably use their natural resources. Mining for natural gases, deforestation, and even improper use of water resources can have tremendous effects on the environment. While these strategies often attempt to boost local economies, their effects can lead to oil spills, interrupted animal habitats, and droughts.

Ultimately, the effects of the modern world on the environment can lead to many problems. Human beings need to consider the repercussions of their actions, trying to reduce, reuse, and recycle materials while establishing environmentally sustainable habits. If measures are not taken to protect the environment, we can potentially witness the extinction of more endangered species, worldwide pollution, and a completely uninhabitable planet.
\section{Appendix 2. Prompts}
\textbf{Prompt. Full Statistics}
\medskip 

You are an expert in english texts complexity evaluation.
Evaluate the text focusing on metrics.
Text: user sample. 
\medskip 

Metrics
\medskip 

General information:
    \begin{enumerate}
        \item The number of sentences in the text
        \item The number of words in the text
        \item The number of words (without stopwords) in the text
        \item The average number of words in the sentence
        \item The average number of words (without stopwords) in the text
    \end{enumerate}
Phonological level:
    \begin{enumerate}
        \item The number of syllables in the text
        \item The average number of syllables in the word
        \item The average number of syllables in the sentence
        \item The number of 1-syllable, 2-syllable, 3-syllable and 4-syllable words in the text
        \item The average number of 1-syllable, 2-syllable, 3-syllable and 4-syllable words in the sentence
    \end{enumerate}
    Grammatical level:
    \begin{enumerate}
        \item The number of nouns in the text. Do not write nouns
        \item The number of adjectives in the text. Do not write adjectives
        \item The number of verbs in the text. Do not write verbs
        \item The number of adverbs in the text. Do not write adverbs
        \item The number of pronouns in the text. Do not write pronouns
        \item The number of numerals in the text. Do not write numerals
        \item Text descriptivity: the number of adjectives divide to the whole number of all words in the text
        \item Text nominativity: the number of nouns divide to the whole number of all words in the text
    \end{enumerate}
        Lexical level:
    \begin{enumerate}
        \item The number of A1, A2, B1, B2, C1 words in the text
        \item The average number of A1, A2, B1, B2, C1 words in the sentences
        \item The number of A1, A2, B1, B2, C1 collocations in the text
        \item The average number of A1, A2, B1, B2, C1 collocations in the sentences
        \item Type token ratio (TTR): TTR = (N / total words)
            where N - the number of all unique words in the text
            total words - the number of all words in the text. 
            For intanse: N = 120, total words = 240. 
            "TTR = N / total words = 120 / 240 = 0.5
        \item Root Type Token Ratio (RTTR)
        \item Corrected Type Token Ratio (CTTR)
    \end{enumerate}
        Topic modeling:
        Name top 5 topics in the text
\medskip

Statistical metrics:
    \begin{enumerate}
        \item Flesh Reading Ease (FRE)
        \item Flesh-Kincaid Grade Level (FKGL)
        \item LIX (Läsbarhetsindex)
        \item SMOG (Simple Measure of Gobbledygook)
    \end{enumerate}
        Write all metrics and a text level according to CEFR (A1, A2, B1, B2, C1, C2) based on these metrics
        If it is a text on another language, say that you could not evaluate it
        Extract named entities (personal names, toponyms, organizations) and write them seperately
        Present the result in a json format
\medskip

\textbf{Prompt. Chain-of-Thought}
\medskip

You are an expert in English texts evaluation.
"Evaluate the English text (find an answer) focusing on metrics. Based your answer on the reasoning below.
"Firstly, find a text language. If it is not an English, say that you could not evaluate it and do not base on reasoning at all
"If it is written in English, evaluate it
"Answers: A1, A2, B1, B2, C1, C2
    Text: {message}
    Metrics:
    \begin{enumerate}
        \item The number of sentences in the text
        \item The number of words in the text
        \item The number of long words (3 syllables and more)
        \item The number of syllables in the text
        \item FRE
        \item FKGL
        \item LIX
        \item SMOG
     \end{enumerate}
Reasoning: Use FRE, FKGL, LIX and SMOG interpretations to choose an answer
If the text is not written in English, write an answer using the format: This text is not written in English
If the text is written in English, write an answer usig the format: This text has <answer> level
And explain in few words (10-15 words) why did you choose this level based on FRE, FKGL, LIX and SMOG interpretations

\section{Appendix 3. Full Comparison. LLM and Script}
Text 1 (A1 level)
\begin{center}
\begin{tabular}{c|c|c}
\textbf{Complexity Metric} & \textbf{LLM (Qwen 2.5)} & \textbf{Script} \\
\hline
\medskip
THE NUMBER OF SENTENCES & 12 & 16 \\
\hline
\medskip
THE NUMBER OF WORDS & 146 & 144 \\
\hline
\medskip
THE NUMBER OF WORDS WITHOUT STOPWORDS & 89 & 74 \\
\hline
\medskip
AVERAGE WORDS IN THE SENTENCE & 12.7 & 9.0 \\
\hline
\medskip
AVERAGE WORDS WITHOUT STOPWORDS IN THE SENTENCE & 7.42 & 4.62 \\
\hline
\medskip
THE NUMBER OF SYLLABLES IN THE TEXT & 215 & 116 \\
\hline
\medskip
THE AVERAGE NUMBER OF SYLLABLES IN THE WORD & 1.47 & 1.57 \\
\hline
\medskip
THE AVERAGE NUMBER OF SYLLABLES IN THE SENTENCE & 17.92 & 7.25 \\
\hline
\medskip
NOMINATIVITY & 0.137 & 0.47 \\
\hline
\medskip
DESCRIPTIVITY & 0.082 & 0.26 \\
\hline
\medskip
A1 WORDS & 65 & 49 \\
\hline
\medskip
A2 WORDS & 45 & 3 \\
\hline
\medskip
B1 WORDS & 30 & 1 \\
\hline
\medskip
B2 WORDS & 6 & 0 \\
\hline
\medskip
C1 WORDS & 0 & 0 \\
\hline
\medskip
TTR & 0.644 & 0.16 \\
\hline
\medskip
RTTR & 11.18 & 1.92 \\
\hline
\medskip
CTTR & 6.88 & 1.36 \\
\hline
\medskip
STATISTICAL METRICS "Flesh Reading Ease (FRE)" & 84.84 & 129.55 \\
\hline
\medskip
Flesh-Kincaid Grade Level (FKGL) & 3.89 & -2.57 \\
\hline
\medskip
LIX (Läsbarhetsindex) & 38.63 & 18.03 \\
\hline
\medskip
SMOG (Simple Measure of Gobbledygook)" & 4.84 & 6.61 \\
\hline
\end{tabular}
\end{center}
\section{Appendix 5. Telegram bot}
\includegraphics[weight=13cm, height=15cm]{bot_1.jpg}
\centering
\par
\capting{Figure 1. Analyzing Text Messages via Telegram bot}
\centering
\medskip

\includegraphics[weight=14cm, height=14cm]{bot_2.jpg}
\medskip
\capting{Figure 2. Analyzing Text Documents via Telegram bot}
\centering
\includegraphics[weight=12cm, height=12cm]{bot_3.jpg}
\medskip

\capting{Figure 3. Analyzing Text Voice Messages via Telegram bot}
\centering
\includegraphics[weight=12cm, height=18cm]{bot_4.jpg}
\centering
\par
\capting{Figure 4. Analyzing Video Messages via Telegram bot}
\centering
\medskip

\end{document}
